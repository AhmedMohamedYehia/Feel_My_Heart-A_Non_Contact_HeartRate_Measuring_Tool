{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Imports:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from matplotlib import style\n",
    "from sklearn.metrics import f1_score\n",
    "style.use('ggplot')\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# $Functions$ $used$ $in$ $module:$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### *Cross Validation*\n",
    "\n",
    "#####  Model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n",
    "\n",
    "\n",
    "#####  To guarantee that the model is not overfitted or the hyperpramters is overfitted too\n",
    "\n",
    "\n",
    "##### Give the chance that each sample get tested on at some point and that reduces bias \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/LOOCV.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_train(clf, x, y, k_folds = 5, return_all = True, print_all = True):\n",
    "    cv_results = cross_validate(clf, x, y, cv=k_folds)\n",
    "    avrg_fit_time = np.sum(cv_results['fit_time'])/k_folds\n",
    "    avrg_score_time = (np.sum(cv_results['score_time'])/k_folds)\n",
    "    avrg_score = (np.sum(cv_results['test_score'])/k_folds)\n",
    "    \n",
    "    if print_all:\n",
    "        print(\"fit_times:   \"+str(cv_results['fit_time'])+\", avrg fit time: \"+str(avrg_fit_time))\n",
    "        print(\"score_times: \"+str(cv_results['score_time'])+\", avrg score time: \"+str(avrg_score_time))\n",
    "        print(\"test_scores: \"+str(cv_results['test_score'])+\", avrg acc: \"+str(avrg_score))\n",
    "    if return_all:\n",
    "        return avrg_fit_time, avrg_score_time,  avrg_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(x1, x2):\n",
    "    distance = np.linalg.norm(x1 - x2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "# $Data$  $manipulation$\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Train-Test Split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# *PCA:*\n",
    "\n",
    "<hr>\n",
    "\n",
    "### PCA from Scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    mu = np.mean(X,axis=0)\n",
    "    sigma = np.std(X,axis=0)\n",
    "    normalized_X = (X-mu)/sigma\n",
    "    \n",
    "    return (normalized_X, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_scratch_pca(X):\n",
    "    # First, we need to compute the covariance matrix of X. (Check np.cov)\n",
    "#     cov = np.cov(X,rowvar=False)\n",
    "    cov = ((X).T@X)/X.shape[0]\n",
    "    # Second, we need to find the eigenvectors of this covariance matrix.\n",
    "    \n",
    "#    Unitary matrix: defined as a square matrix whose conjugate transpose is also its inverse. \n",
    "#   u eigen vectors matrix \n",
    "#   s eigen values\n",
    "    u,s,_ = np.linalg.svd(cov)\n",
    "    return u,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectData(X, U, K):\n",
    "    Z = -1* X @ U[:,:K]\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "### Testing against PCA from Library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, -1],\n",
       "       [-2, -1, -1, -1],\n",
       "       [-3, -2, -1, -2],\n",
       "       [ 1,  1,  2,  2],\n",
       "       [ 2,  1,  4,  2],\n",
       "       [ 3,  2,  2,  5]])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[-1, -1,-1,-1], [-2, -1,-1,-1], [-3, -2,-1,-2], [1, 1,2,2], [2, 1,4,2], [3, 2,2,5]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42367278, -0.2405279 ,  0.30244267],\n",
       "       [-1.66181164, -0.19605803, -0.06708039],\n",
       "       [-2.47143094,  0.19848579, -0.17244311],\n",
       "       [ 1.1246693 ,  0.08069594, -0.07043853],\n",
       "       [ 1.84340642,  0.90678962,  0.06348409],\n",
       "       [ 2.58883964, -0.74938541, -0.05596473]])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_pca_normed = scaler.fit_transform(x)\n",
    "\n",
    "pca.fit_transform(x_pca_normed)\n",
    "\n",
    "# pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=3))])\n",
    "# pipeline.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42367278, -0.2405279 ,  0.30244267],\n",
       "       [-1.66181164, -0.19605803, -0.06708039],\n",
       "       [-2.47143094,  0.19848579, -0.17244311],\n",
       "       [ 1.1246693 ,  0.08069594, -0.07043853],\n",
       "       [ 1.84340642,  0.90678962,  0.06348409],\n",
       "       [ 2.58883964, -0.74938541, -0.05596473]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm, mu, sigma = featureNormalize(x)\n",
    "test_u, test_s = from_scratch_pca(X_norm)\n",
    "z_test = projectData(X_norm, test_u, 3)\n",
    "z_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "# $Machine Learning  Models$:\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## Built from Scratch Models:\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinimumDistanceClassifier(test_point, training_features, labels):\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # INPUTS:   test_point: (1, N) where n is the number of features. \n",
    "    #           training_features: (M, N) array where M is the training set size, and N is the number of features.\n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    unique_labels = labels.unique()\n",
    "    feature_dict = {}\n",
    "    for u_label in unique_labels:\n",
    "        feature_dict[u_label] = training_features[labels == u_label]\n",
    "    \n",
    "    for u_label in unique_labels:\n",
    "        feature_dict[u_label] = np.mean(feature_dict[u_label],axis=0)\n",
    "        \n",
    "    minimum = 99999999999999\n",
    "    classification = -1\n",
    "    for key, value in feature_dict.items():\n",
    "        if minimum >  calculateDistance(np.asarray(test_point),np.asarray(value)):\n",
    "            minimum = calculateDistance(np.asarray(test_point),np.asarray(value))\n",
    "            classification = key\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NearestNeighbor(test_point, training_features, labels):\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # INPUTS:   test_point: (1, N) where N is the number of features. \n",
    "    #           training_features: (M, N) array where M is the training set size, and N is the number of features.\n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (Rectangle), 1 (Circle), or 2 (Triangle)\n",
    "    classification = -1\n",
    "    minimum = 9999999999\n",
    "    for i in range(len(training_features)) :\n",
    "        if minimum > calculateDistance(np.asarray(test_point),np.asarray(training_features.iloc[i,:])):\n",
    "            minimum = calculateDistance(np.asarray(test_point),np.asarray(training_features.iloc[i,:]))\n",
    "            classification = labels[i]\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(test_point, training_features, k, labels):\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # INPUTS:   test_point: (1, N) where N is the number of features. \n",
    "    #           training_features: (M, N) array where M is the training set size, and N is the number of features.\n",
    "    #           k: the number of nearest neighbours. \n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (Rectangle), 1 (Circle), or 2 (Triangle)    \n",
    "    \n",
    "    dists = []\n",
    "    for i in range(len(training_features)):\n",
    "        dists.append(calculateDistance(np.asarray(test_point),np.asarray(training_features.iloc[i,:])))\n",
    "        \n",
    "    dists = np.asarray(dists)\n",
    "    sorted_dists_indices = dists.argsort()[:k] \n",
    "    \n",
    "    counts = {}\n",
    "    for u_label in labels.unique():\n",
    "        counts[u_label] = (np.count_nonzero(labels[sorted_dists_indices] == u_label))\n",
    "        \n",
    "    classification = max(counts, key=counts.get)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SVM(object):\n",
    "#     def __init__(self,visualization=True):\n",
    "#         self.visualization = visualization\n",
    "#         self.colors = {1:'r',-1:'b'}\n",
    "#         if self.visualization:\n",
    "#             self.fig = plt.figure()\n",
    "#             self.ax = self.fig.add_subplot(1,1,1)\n",
    "    \n",
    "#     def fit(self,data):\n",
    "#         #train with data\n",
    "#         self.data = data\n",
    "#         # { |\\w\\|:{w,b}}\n",
    "#         opt_dict = {}\n",
    "        \n",
    "#         transforms = [[1,1],[-1,1],[-1,-1],[1,-1]]\n",
    "        \n",
    "#         all_data = np.array([])\n",
    "#         for yi in self.data:\n",
    "#             all_data = np.append(all_data,self.data[yi])\n",
    "                    \n",
    "#         self.max_feature_value = max(all_data)         \n",
    "#         self.min_feature_value = min(all_data)\n",
    "#         all_data = None\n",
    "        \n",
    "#         #with smaller steps our margins and db will be more precise\n",
    "#         step_sizes = [self.max_feature_value * 0.1,\n",
    "#                       self.max_feature_value * 0.01,\n",
    "#                       #point of expense\n",
    "#                       self.max_feature_value * 0.001,]\n",
    "        \n",
    "#         #extremly expensise\n",
    "#         b_range_multiple = 5\n",
    "#         #we dont need to take as small step as w\n",
    "#         b_multiple = 5\n",
    "        \n",
    "#         latest_optimum = self.max_feature_value*10\n",
    "        \n",
    "#         \"\"\"\n",
    "#         objective is to satisfy yi(x.w)+b>=1 for all training dataset such that ||w|| is minimum\n",
    "#         for this we will start with random w, and try to satisfy it with making b bigger and bigger\n",
    "#         \"\"\"\n",
    "#         #making step smaller and smaller to get precise value\n",
    "#         for step in step_sizes:\n",
    "#             w = np.array([latest_optimum,latest_optimum])\n",
    "            \n",
    "#             #we can do this because convex\n",
    "#             optimized = False\n",
    "#             while not optimized:\n",
    "#                 for b in np.arange(-1*self.max_feature_value*b_range_multiple,\n",
    "#                                    self.max_feature_value*b_range_multiple,\n",
    "#                                    step*b_multiple):\n",
    "#                     for transformation in transforms:\n",
    "#                         w_t = w*transformation\n",
    "#                         found_option = True\n",
    "                        \n",
    "#                         #weakest link in SVM fundamentally\n",
    "#                         #SMO attempts to fix this a bit\n",
    "#                         # ti(xi.w+b) >=1\n",
    "#                         for i in self.data:\n",
    "#                             for xi in self.data[i]:\n",
    "#                                 yi=i\n",
    "#                                 if not yi*(np.dot(w_t,xi)+b)>=1:\n",
    "#                                     found_option=False\n",
    "#                         if found_option:\n",
    "#                             \"\"\"\n",
    "#                             all points in dataset satisfy y(w.x)+b>=1 for this cuurent w_t, b\n",
    "#                             then put w,b in dict with ||w|| as key\n",
    "#                             \"\"\"\n",
    "#                             opt_dict[np.linalg.norm(w_t)]=[w_t,b]\n",
    "                \n",
    "#                 #after w[0] or w[1]<0 then values of w starts repeating itself because of transformation\n",
    "#                 #Think about it, it is easy\n",
    "#                 #print(w,len(opt_dict)) Try printing to understand\n",
    "#                 if w[0]<0:\n",
    "#                     optimized=True\n",
    "#                     print(\"optimized a step\")\n",
    "#                 else:\n",
    "#                     w = w-step\n",
    "                    \n",
    "#             # sorting ||w|| to put the smallest ||w|| at poition 0 \n",
    "#             norms = sorted([n for n in opt_dict])\n",
    "#             #optimal values of w,b\n",
    "#             opt_choice = opt_dict[norms[0]]\n",
    "\n",
    "#             self.w=opt_choice[0]\n",
    "#             self.b=opt_choice[1]\n",
    "            \n",
    "#             #start with new latest_optimum (initial values for w)\n",
    "#             latest_optimum = opt_choice[0][0]+step*2\n",
    "    \n",
    "#     def predict(self,features):\n",
    "#         #sign(x.w+b)\n",
    "#         classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
    "#         if classification!=0 and self.visualization:\n",
    "#             self.ax.scatter(features[0],features[1],s=200,marker='*',c=self.colors[classification])\n",
    "#         return (classification,np.dot(np.array(features),self.w)+self.b)\n",
    "    \n",
    "#     def visualize(self):\n",
    "#         [[self.ax.scatter(x[0],x[1],s=100,c=self.colors[i]) for x in data_dict[i]] for i in data_dict]\n",
    "        \n",
    "#         # hyperplane = x.w+b (actually its a line)\n",
    "#         # v = x0.w0+x1.w1+b -> x1 = (v-w[0].x[0]-b)/w1\n",
    "#         #psv = 1     psv line ->  x.w+b = 1a small value of b we will increase it later\n",
    "#         #nsv = -1    nsv line ->  x.w+b = -1\n",
    "#         # dec = 0    db line  ->  x.w+b = 0\n",
    "#         def hyperplane(x,w,b,v):\n",
    "#             #returns a x2 value on line when given x1\n",
    "#             return (-w[0]*x-b+v)/w[1]\n",
    "       \n",
    "#         hyp_x_min= self.min_feature_value*0.9\n",
    "#         hyp_x_max = self.max_feature_value*1.1\n",
    "        \n",
    "#         # (w.x+b)=1\n",
    "#         # positive support vector hyperplane\n",
    "#         pav1 = hyperplane(hyp_x_min,self.w,self.b,1)\n",
    "#         pav2 = hyperplane(hyp_x_max,self.w,self.b,1)\n",
    "#         self.ax.plot([hyp_x_min,hyp_x_max],[pav1,pav2],'k')\n",
    "        \n",
    "#         # (w.x+b)=-1\n",
    "#         # negative support vector hyperplane\n",
    "#         nav1 = hyperplane(hyp_x_min,self.w,self.b,-1)\n",
    "#         nav2 = hyperplane(hyp_x_max,self.w,self.b,-1)\n",
    "#         self.ax.plot([hyp_x_min,hyp_x_max],[nav1,nav2],'k')\n",
    "        \n",
    "#         # (w.x+b)=0\n",
    "#         # db support vector hyperplane\n",
    "#         db1 = hyperplane(hyp_x_min,self.w,self.b,0)\n",
    "#         db2 = hyperplane(hyp_x_max,self.w,self.b,0)\n",
    "#         self.ax.plot([hyp_x_min,hyp_x_max],[db1,db2],'y--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = {-1:np.array([[1,7,10],[2,8,11],[3,8,9]]),1:np.array([[5,1,-10],[6,-1,-9],[7,3,-11]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVM() # Linear Kernel\n",
    "# svm.fit(data=data_dict)\n",
    "# # svm.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm.predict([3,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm.predict([6,6.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "# $Testing$ $Models:$\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Reading Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"D:\\Uni\\GP\\Machine_Learning_Module\\Data\\points_with_classes\\data1.csv\",header=None)\n",
    "x_train = train_data.iloc[:,1:]\n",
    "y_train = train_data.iloc[:,0]\n",
    "\n",
    "x_test = pd.read_csv(r\"D:\\Uni\\GP\\Machine_Learning_Module\\Data\\points_with_classes\\test_data.csv\",header=None)\n",
    "y_test = pd.read_csv(r\"D:\\Uni\\GP\\Machine_Learning_Module\\Data\\points_with_classes\\test_data_true.csv\",header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1          2\n",
      "0  3.272089   0.726774\n",
      "1 -6.937777  -6.449828\n",
      "2  6.056029   0.487195\n",
      "3 -2.347665  -3.819760\n",
      "4 -1.148770  12.649768\n",
      "0    3.0\n",
      "1    1.0\n",
      "2    3.0\n",
      "3    1.0\n",
      "4    2.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(x_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1\n",
      "0  10.701414  3.872536\n",
      "1  -3.818318 -5.009778\n",
      "2  -3.570719  9.960362\n",
      "3   4.943090 -0.015394\n",
      "4   4.260826 -0.613494\n",
      "     0\n",
      "0  3.0\n",
      "1  1.0\n",
      "2  2.0\n",
      "3  3.0\n",
      "4  3.0\n"
     ]
    }
   ],
   "source": [
    "print(x_test.head())\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Testing Built from Scratch Models:\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Minimum Distance Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "for i in range(len(y_test)):\n",
    "    classifications.append(MinimumDistanceClassifier(x_test.iloc[i,:],x_train,y_train)) # rect 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9601486567986924"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = np.asarray(classifications)\n",
    "y_test = np.asarray(y_test)\n",
    "f1_score(y_test,classifications,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "#### Nearst Neighbor Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "classifications = []\n",
    "# for i in range(len(y_test)):\n",
    "count = 0\n",
    "sum_correct = 0\n",
    "for i in  range(len(y_test)):\n",
    "    classifications.append(NearestNeighbor(x_test.iloc[i,:],x_train,y_train)) # rect 0\n",
    "    count +=1\n",
    "    if classifications[i] == y_test[i]:\n",
    "        sum_correct +=1\n",
    "print(sum_correct / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### K-Nearst Neighbor Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "for i in range(len(y_test)):\n",
    "    classifications.append(KNN(x_test.iloc[i,:],x_train,15,y_train)) # rect 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644732852324301"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = np.asarray(classifications)\n",
    "y_test = np.asarray(y_test)\n",
    "f1_score(y_test,classifications,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Testing Library Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535259648413822"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(x_train, y_train)\n",
    "f1_score(y_test,neigh.predict(x_test),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644732852324301"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=15)\n",
    "neigh.fit(x_train, y_train)\n",
    "f1_score(y_test,neigh.predict(x_test),average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644732852324301"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(gamma=0.1,C=0.2)\n",
    "clf.fit(x_train, y_train)\n",
    "f1_score(y_test,clf.predict(x_test),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9578488063406323"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=1, max_iter=200).fit(x_train, y_train)\n",
    "clf.fit(x_train, y_train)\n",
    "f1_score(y_test,clf.predict(x_test),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9513037731314588"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators =1000,max_depth=None, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "f1_score(y_test,clf.predict(x_test),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.960106365638179"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(multi_class='multinomial', random_state=1)\n",
    "clf.fit(x_train, y_train)\n",
    "f1_score(y_test,clf.predict(x_test),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stacking Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644732852324301"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "#      ('rf', RandomForestClassifier(n_estimators =1000,max_depth=None, random_state=0)),\n",
    "     ('knn', KNeighborsClassifier(n_neighbors=15)),\n",
    "     ('svm',SVC(gamma=0.1,C=0.2))\n",
    " ]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "clf.fit(x_train, y_train)\n",
    "f1_score(y_test,clf.predict(x_test),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Majority Vote Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644732852324301"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
    "# clf2 = RandomForestClassifier(n_estimators=1000, random_state=1)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=15)\n",
    "clf4 = SVC(gamma=0.1,C=0.2)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('gnb', clf3), ('rof', clf4)], voting='hard')\n",
    "eclf1 = eclf1.fit(x_train, y_train)\n",
    "f1_score(y_test,eclf1.predict(x_test),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
